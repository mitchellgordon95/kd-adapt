##################################################################################################
# Packages used
##################################################################################################

package sockeye :: .versioner=git .repo="https://github.com/mjpost/sockeye" .ref=HEAD { }
package sacrebleu :: .versioner=pip .package="sacrebleu" .tag="1.2.12" { }
package subword_nmt :: .versioner=pip .package="subword-nmt" .tag="0.3.5" { }
package mosesdecoder :: .versioner=git .repo="https://github.com/moses-smt/mosesdecoder" .ref=HEAD { }

package sentencepiece :: .versioner=git .repo="https://github.com/google/sentencepiece" .ref="tags/v0.1.5" {  # v0.1.6 throws segfault
  mkdir build
  cd build
  cmake ..
  make -j $(nproc)
}

package tools
    :: .versioner=git .repo="https://github.com/shuoyangd/tape4nmt-tools" .ref=HEAD {
  pip install -r requirements.txt
}

# using my fork for now, as fairseq evolves pretty fast
package fairseq
    :: .versioner=git .repo="https://github.com/shuoyangd/fairseq" .ref=HEAD {

  python setup.py build develop
}

global {

  # Language choices
  SRC=(Lang:
    deen="de"
    ruen="ru"
  )
  TRG=(Lang:
    deen="en"
    ruen="en"
  )

  domain=(Domain:
    wipo="wipo"
    ted="ted"
  )

  # Architecture choices
  # TODO: maybe vary this with choice of language
  teacher_layers=6
  teacher_FF_size=2048
  teacher_model_size=512
  teacher_embed_size="512:512"

  # TODO: do a big student and a small student branch
  student_layers=(StudentSize:
    half=3
    quarter=3
  )
  student_FF_size=(StudentSize:
    half=2048
    quarter=1024
  )
  student_model_size=(StudentSize:
    half=512
    quarter=256
  )
  student_embed_size=(StudentSize:
    half="512:512"
    quarter="256:256"
  )

  ##################################################################################################
  # General options you should set for your environment
  ##################################################################################################

  # All ducttape files will be written underneath this directory
  ducttape_output="out"

  # Data pre-processing
  MaxLen=100
  Ratio=1

  ##################################################################################################
  # Job submission parameters
  ##################################################################################################

  # SGE: generic job flags
  resource_flags="-l mem_free=2g"

  # SGE: larger job flags
  resource_flags_16g="-l mem_free=16g"

  # SGE: flags for training a model
  resource_flags_train="-q gpu.q@@2080 -l gpu=1,mem_free=4g,h_rt=120:00:00"

  # SGE: flags for decoding
  resource_flags_decode="-q gpu.q@@2080 -l gpu=1,mem_free=4g,h_rt=120:00:00"

  # SGE: flags for notifying about job completion (put in your email address!)
  action_flags="-m ae -M mitchell.gordon95@gmail.com"

  # The default submitter: shell (run locally) or sge (run on a grid)
  submitter=(TestMode: no="sge" yes="shell")

  use_cpu=(TestMode: no yes)

  # Virtual env location. This should be a file path to the virtual env you want loaded before tasks.
  # This variable supports both conda and Python's virtualenv. For conda, use "conda:ENV" as the value,
  # where "ENV" is the name of the conda environment that should be loaded. For virtualenv, supply
  # the path to the script that should be loaded.
  pyenv="conda:sockeye"
}
